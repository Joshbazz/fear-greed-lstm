{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome! All code included in the Python Implementation is also included here for ease of use. You can run this entire notebook from start to finish, and look at generated console outputs, and visualizations generated as saved PNGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cell below gathers Fear and Index Data, and combines it with Bitcoin price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "!pip install yfinance\n",
    "import yfinance as yf\n",
    "\n",
    "\n",
    "def fetch_fear_and_greed_btc():\n",
    "    # Define the API endpoint and parameters for Fear and Greed Index\n",
    "    fng_api_url = \"https://api.alternative.me/fng/\"\n",
    "    fng_params = {\n",
    "        'limit': 0,  # Get all available data\n",
    "        'format': 'json'\n",
    "    }\n",
    "\n",
    "    # Make the GET request to the Fear and Greed Index API\n",
    "    response = requests.get(fng_api_url, params=fng_params)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        fng_data = response.json()\n",
    "        # Convert the data to a Pandas DataFrame\n",
    "        fng_df = pd.DataFrame(fng_data['data'])\n",
    "        # Ensure the timestamp column is of numeric type before converting to datetime\n",
    "        fng_df['timestamp'] = pd.to_numeric(fng_df['timestamp'], errors='coerce')\n",
    "        # Convert the timestamp column to datetime\n",
    "        fng_df['timestamp'] = pd.to_datetime(fng_df['timestamp'], unit='s')\n",
    "        # Drop the time_until_update column\n",
    "        fng_df.drop(columns=['time_until_update'], inplace=True)\n",
    "        # Set the timestamp as the index\n",
    "        fng_df.set_index('timestamp', inplace=True)\n",
    "        # Sort the DataFrame by the index (timestamp) in ascending order\n",
    "        fng_df.sort_index(inplace=True)\n",
    "        # Save the Fear and Greed Index DataFrame to a CSV file with timestamp as index and column name 'timestamp'\n",
    "        fng_df.to_csv('fear_and_greed_index.csv', index=True, index_label='timestamp')\n",
    "        print(\"Fear and Greed Index data has been saved to 'fear_and_greed_index.csv'.\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch Fear and Greed Index data. Status code: {response.status_code}\")\n",
    "\n",
    "    # Fetch daily Bitcoin prices using Yahoo Finance\n",
    "    btc_data = yf.download('BTC-USD', start=fng_df.index.min().strftime('%Y-%m-%d'), end=fng_df.index.max().strftime('%Y-%m-%d'))\n",
    "\n",
    "    # Concatenate Fear and Greed Index DataFrame with Bitcoin DataFrame based on date index\n",
    "    combined_df = pd.concat([fng_df, btc_data], axis=1, join='inner')\n",
    "\n",
    "    # Save the combined DataFrame to a CSV file\n",
    "    combined_df.to_csv('fear_greed_btc_combined.csv', index=True, index_label='timestamp')\n",
    "    print(\"Combined data has been saved to 'fear_greed_btc_combined.csv'.\")\n",
    "\n",
    "    return combined_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code below contains a DataPreprocessor Class for getting data ready for use by our LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, X_scaler=RobustScaler(), y_scaler=RobustScaler(), lag_features=['value', 'Close'], lags=5, target_col='Close', test_size=.25, window_size=5):\n",
    "        self.lag_features = lag_features\n",
    "        self.lags = lags\n",
    "        self.window_size = window_size\n",
    "        self.target_col = target_col\n",
    "        self.test_size = test_size\n",
    "        self.X_scaler = X_scaler\n",
    "        self.y_scaler = y_scaler\n",
    "\n",
    "    def create_lagged_features(self, df):\n",
    "        for feature in self.lag_features:\n",
    "            for lag in range(1, self.lags + 1):\n",
    "                df[f'{feature}_lag_{lag}'] = df[feature].shift(lag)\n",
    "        df['target'] = df[self.target_col]\n",
    "        df.dropna(inplace=True)\n",
    "        return df\n",
    "\n",
    "    def convert_to_window_format(self, df):\n",
    "        X, y, dates = [], [], []\n",
    "        for i in range(len(df) - self.window_size):\n",
    "            window = df.iloc[i:i+self.window_size]\n",
    "            X.append(window.drop(columns=['target', 'Close', 'Adj Close']).values)\n",
    "            # Append the Close price of the last day in the window as the target\n",
    "            y.append(window.iloc[-1]['Close'])\n",
    "            dates.append(window.index[-1]) # store the date of the last row in the window\n",
    "        self.X, self.y = np.array(X), np.array(y)\n",
    "        self.dates = np.array(dates)\n",
    "\n",
    "        return self.X, self.y, self.dates\n",
    "\n",
    "    def normalize_data(self, X_train, X_test, y_train, y_test):\n",
    "        # Reshape X_train and X_test to fit_transform\n",
    "        X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "        X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])\n",
    "\n",
    "        X_train_scaled = self.X_scaler.fit_transform(X_train_reshaped)\n",
    "        X_test_scaled = self.X_scaler.transform(X_test_reshaped)\n",
    "        \n",
    "        # Reshape back to original shape\n",
    "        X_train_scaled = X_train_scaled.reshape(X_train.shape)\n",
    "        X_test_scaled = X_test_scaled.reshape(X_test.shape)\n",
    "\n",
    "        y_train = y_train.reshape(-1, 1)\n",
    "        y_train_scaled = self.y_scaler.fit_transform(y_train)\n",
    "        y_test = y_test.reshape(-1, 1)\n",
    "        y_test_scaled = self.y_scaler.transform(y_test)\n",
    "\n",
    "        return X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled\n",
    "\n",
    "    def split_train_test(self, data):\n",
    "        lagged_df = self.create_lagged_features(data)\n",
    "        lagged_df = lagged_df.drop(columns=['value_classification'])\n",
    "\n",
    "        self.X, self.y, self.dates = self.convert_to_window_format(lagged_df)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=self.test_size, shuffle=False)\n",
    "\n",
    "        X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = self.normalize_data(\n",
    "            X_train, X_test, y_train, y_test\n",
    "        )\n",
    "        self.dates_train = self.dates[:len(y_train_scaled)]\n",
    "        self.dates_test = self.dates[len(y_train_scaled):]\n",
    "        self.features_test_df = lagged_df[-len(self.dates_test)-1:] #NOTE this could cause issues\n",
    "\n",
    "        return X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, X_test, y_test, self.dates_train, self.dates_test, self.features_test_df\n",
    "\n",
    "    def preprocess_data(self, data):\n",
    "        return self.split_train_test(data)\n",
    "\n",
    "    def inverse_transform_y(self, y_scaled):\n",
    "        return self.y_scaler.inverse_transform(y_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the Signal Generation function. Update the code here to implement a new strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from datetime import datetime\n",
    "\n",
    "def generate_signal(test_features, predictions, dates_test, model_path=None):\n",
    "\n",
    "    # put your predictions vector back into the test features dataframe\n",
    "    dates_test_reshaped = dates_test.reshape(-1, 1)\n",
    "\n",
    "    combined_array = np.concatenate((dates_test_reshaped, predictions), axis=1)\n",
    "    # print(combined_array[:10])\n",
    "\n",
    "    df_combined = pd.DataFrame(combined_array, columns=['Date', 'Predicted_Close'])\n",
    "    df_combined.set_index('Date', inplace=True)\n",
    "\n",
    "    result_df = pd.concat([test_features, df_combined], axis=1)\n",
    "\n",
    "####################################################################################\n",
    "#------------------------CREATE YOUR STRATEGY HERE---------------------------------#\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "    # Initialize an empty list to store signals\n",
    "    signals = []\n",
    "\n",
    "    # Iterate through each row of the DataFrame\n",
    "    for i in range(len(result_df) - 1):\n",
    "        open = result_df['Open'].iloc[i]\n",
    "        prediction = result_df['Predicted_Close'].iloc[i]\n",
    "        \n",
    "        # Define your buy and sell conditions here (modular and editable)\n",
    "        if open < prediction:\n",
    "            signal = 1  # Buy signal\n",
    "        else:\n",
    "            signal = -1  # Sell signal\n",
    "        \n",
    "        signals.append(signal)\n",
    "\n",
    "    # Handle the last element if necessary\n",
    "    if len(signals) < len(test_features):\n",
    "        signals.append(None)\n",
    "\n",
    "    # Add the signals list as a new column 'signal' in the DataFrame\n",
    "    test_features['Signal'] = signals\n",
    "\n",
    "    # Get the current timestamp and format it\n",
    "    current_timestamp = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "    # Define the full path for the new CSV file\n",
    "    csv_path = os.path.join(f'{current_timestamp}_new_data_with_positions.csv')\n",
    "\n",
    "    # Save new_data with positions\n",
    "    test_features.to_csv(csv_path, index=True)\n",
    "\n",
    "    return test_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the ModelEvaluator Class, used for evaluating our model's predictive capabilities on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, model, X_test, y_test, X_test_scaled, y_test_scaled, y_scaler, model_path=None):\n",
    "        self.model = model\n",
    "        self.model_path = model_path\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.X_test_scaled = X_test_scaled\n",
    "        self.y_test_scaled = y_test_scaled\n",
    "        self.y_scaler = y_scaler\n",
    "        self.predictions = None\n",
    "        self.predictions_inversed = None\n",
    "        self.y_test_inversed = None\n",
    "\n",
    "        if not self.model and model_path:\n",
    "            self.load_saved_model(model_path)    \n",
    "    \n",
    "    def load_saved_model(self, model_path):\n",
    "        self.model = load_model(model_path)\n",
    "        print(f'Model loaded from {model_path}')\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        loss, mae = self.model.evaluate(self.X_test_scaled, self.y_test_scaled, verbose=2)\n",
    "        error_in_dollars = self.y_test.mean() * mae\n",
    "        print(f'Test Loss: {loss:.4f}')\n",
    "        print(f'Test MAE: {mae:.2f}')\n",
    "        print(f'MAE in dollars: +/- ${error_in_dollars:.2f}')\n",
    "\n",
    "    def atr_to_data(self, window=30):\n",
    "        self.X_test['ATR'] = self.calculate_atr()\n",
    "        atr_total_test = self.X_test['ATR'].mean()\n",
    "        atr_last_window = self.X_test['ATR'].iloc[-window:].mean()\n",
    "        print(f\"ATR for all test observations: ${atr_total_test:.2f}\")\n",
    "        print(f\"ATR for last {window} observations: ${atr_last_window:.2f}\")\n",
    "\n",
    "    def calculate_atr(self, window=14):\n",
    "        high_low = self.X_test['High'] - self.X_test['Low']\n",
    "        high_close_prev = abs(self.X_test['High'] - self.X_test['Close'].shift(1))\n",
    "        low_close_prev = abs(self.X_test['Low'] - self.X_test['Close'].shift(1))\n",
    "\n",
    "        tr = high_low.to_frame(name='HL')\n",
    "        tr['HC_prev'] = high_close_prev\n",
    "        tr['LC_prev'] = low_close_prev\n",
    "\n",
    "        true_range = tr.max(axis=1)\n",
    "\n",
    "        atr = true_range.rolling(window=window, min_periods=1).mean()\n",
    "\n",
    "        return atr\n",
    "        \n",
    "    def predict_model(self):\n",
    "        self.predictions = self.model.predict(self.X_test_scaled)\n",
    "        self.predictions_inversed = self.y_scaler.inverse_transform(self.predictions)\n",
    "        self.y_test_inversed = self.y_scaler.inverse_transform(self.y_test_scaled)\n",
    "        plot_predicted_actual(self.y_test_inversed, self.predictions_inversed)\n",
    "        plot_residuals(self.y_test_inversed, self.predictions_inversed)\n",
    "\n",
    "        return self.predictions_inversed\n",
    "\n",
    "    def generate_model_signals(self):\n",
    "        self.X_test = generate_signal(self.X_test_scaled, self.predictions_inversed)\n",
    "        print(self.X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are the functions for generating the visualizations we will see when our LSTMModel trains and tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "def plot_loss_training_history(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    save_plot(\"loss_training_history\")\n",
    "\n",
    "def plot_mae_training_history(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['mean_absolute_error'], label='Training MAE')\n",
    "    plt.plot(history.history['val_mean_absolute_error'], label='Validation MAE')\n",
    "    plt.title('Model MAE Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    save_plot(\"MAE_training_history\")\n",
    "\n",
    "def plot_predicted_actual(actual, predicted):\n",
    "    # Flatten the 2D arrays to 1D\n",
    "    actual = np.ravel(actual)\n",
    "    predicted = np.ravel(predicted)\n",
    "\n",
    "    df = pd.DataFrame({'Actual': actual, 'Predicted': predicted})\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x='Actual', y='Predicted', data=df)\n",
    "    plt.plot([min(actual), max(actual)], [min(actual), max(actual)], color='red', linestyle='--')\n",
    "    plt.title('Predicted vs. Actual Values')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.grid(True)\n",
    "    save_plot(\"Predicted_vs_Actual\")\n",
    "\n",
    "def plot_residuals(actual, predicted):\n",
    "    # Flatten the 2D arrays to 1D\n",
    "    actual = np.ravel(actual)\n",
    "    predicted = np.ravel(predicted)\n",
    "    residuals = actual - predicted\n",
    "\n",
    "    # residuals = [actual - predicted for actual, predicted in zip(actual, predicted)]\n",
    "    df = pd.DataFrame({'Actual': actual, 'Predicted': predicted, 'Residuals': residuals})\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x='Predicted', y='Residuals', data=df)\n",
    "    plt.axhline(y=0, color='red', linestyle='--')\n",
    "    plt.title('Residuals Plot')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.grid(True)\n",
    "    save_plot(\"Residuals\")\n",
    "\n",
    "def save_and_visualize_model(model, img_dir=None):\n",
    "    timestamp = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    if img_dir is None:\n",
    "        img_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    os.makedirs(img_dir, exist_ok=True)\n",
    "    img_path = os.path.join(img_dir, f\"model_{timestamp}.png\")\n",
    "    plot_model(\n",
    "        model,\n",
    "        to_file=img_path,\n",
    "        show_shapes=True,\n",
    "        show_dtype=False,\n",
    "        show_layer_names=True,\n",
    "        rankdir=\"TB\",\n",
    "        expand_nested=False,\n",
    "        dpi=200,\n",
    "        show_layer_activations=True,\n",
    "        show_trainable=True\n",
    "    )\n",
    "    print(f\"Model visualization saved and displayed from {img_path}\")\n",
    "    # save_plot(\"Model_Arc\")\n",
    "\n",
    "def plot_PCA(X_scaled):\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=2)  # Reduce to 2 dimensions\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "    # Plot PCA results\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
    "    plt.title('PCA Visualization')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.grid(True)\n",
    "    # plt.show()\n",
    "\n",
    "    save_plot(\"PCA_graph\")\n",
    "\n",
    "\n",
    "def save_plot(plot_name):\n",
    "    current_timestamp = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    plt.savefig(f'{plot_name}_{current_timestamp}.png')\n",
    "    plt.close()\n",
    "    print(f\"{plot_name} plot saved as '{plot_name}_{current_timestamp}.png'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the Backtesting code so we can test the efficacy of our predictions and strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "!pip install backtesting\n",
    "from backtesting import Backtest, Strategy\n",
    "    \n",
    "class SignalStrategy(Strategy):\n",
    "    def init(self):\n",
    "        self.signal = self.data.Signal\n",
    "\n",
    "    def next(self):\n",
    "        current_signal = self.data.Signal[-1]\n",
    "        current_date = self.data.index[-1]\n",
    "        # print(f\"Date: {current_date}, Current position size: {self.position.size}, Signal: {current_signal}, Position: {self.position.is_long}\")\n",
    "        \n",
    "        if current_signal == 1:\n",
    "            # print(\"Executing BUY order\")\n",
    "            self.buy(size=1)\n",
    "        elif current_signal == -1 and self.position.is_long:\n",
    "            # print(\"Attempting to SELL entire position\")\n",
    "            try:\n",
    "                self.position.close()  # This closes the entire position\n",
    "                # print(\"SELL order executed - entire position closed\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error executing SELL order: {e}\")\n",
    "        elif current_signal == 0:\n",
    "            # print(\"No trade executed\")\n",
    "            pass\n",
    "    \n",
    "        \n",
    "        # print(f\"Current position size: {self.position.size}\")\n",
    "\n",
    "\n",
    "def run_backtest(data_path=None, data=None, plot=True, cash=1_000_000, commission=0.002, trade_on_close=True):\n",
    "    if data_path:\n",
    "        # Load and preprocess the data from the specified path\n",
    "        dataframe = pd.read_csv(data_path, index_col='Date', parse_dates=True)\n",
    "        dataframe = dataframe.sort_index()\n",
    "        dataframe = dataframe.dropna()\n",
    "        dataframe = dataframe.drop_duplicates()\n",
    "        dataframe.columns = [column.capitalize() for column in dataframe.columns]\n",
    "    elif data is not None:\n",
    "        # Use self.data if called from LSTMModel instance\n",
    "        dataframe = data  # Assuming `self.data` is defined in LSTMModel\n",
    "    \n",
    "    # Initialize and run the backtest\n",
    "    bt = Backtest(dataframe, SignalStrategy, cash=cash, commission=commission, trade_on_close=trade_on_close)\n",
    "    stats = bt.run()\n",
    "\n",
    "    # Print the statistics and plot the backtest results\n",
    "    print(stats)\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    stats_output_file = f'backtest_stats_{current_time}.txt'\n",
    "\n",
    "    # Save the statistics to a text file if stats_output_file is provided\n",
    "    with open(stats_output_file, 'w') as f:\n",
    "        f.write(str(stats))\n",
    "\n",
    "    if plot == True:\n",
    "        bt.plot()\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Main Brain of the code, the LSTMModel Class. This is where your entire model will use all the modules above to create, train and test an LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "class LSTMModel:\n",
    "    def __init__(self, model_path=None, data_path=None, lags=5, test_size=.25, learning_rate = 0.001, epochs=50, batch_size=32, validation_split=0.2, plot=True):\n",
    "        self.model = None\n",
    "        self.model_path = model_path\n",
    "        self.history = None\n",
    "        self.data_path = data_path\n",
    "        self.lag_features = ['value', 'Close'] # change these if you want to calculate lags on different feature columns\n",
    "        self.target_col = 'Close' # change this if you want to target a different variable than Close\n",
    "        self.X_scaler = RobustScaler()\n",
    "        self.y_scaler = RobustScaler()\n",
    "        self.preprocessor = DataPreprocessor(self.X_scaler, self.y_scaler, self.lag_features, lags, self.target_col, test_size)\n",
    "        self.current_timestamp = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "        self.learning_rate = learning_rate \n",
    "        self.loss = 'mean_squared_error' # change this if you're not going to solve for a regression target\n",
    "        self.metrics = ['mean_absolute_error']  # change this if you're not going to solve for a regression target\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_split = validation_split\n",
    "\n",
    "        self.plot = plot # set plot to false when instantiating if you dont want the backtest graph\n",
    "\n",
    "        if model_path:\n",
    "            self.load_saved_model(model_path)\n",
    "    \n",
    "    def load_saved_model(self, model_path):\n",
    "        self.model = load_model(model_path)\n",
    "        print(f'Model loaded from {model_path}')\n",
    "\n",
    "    # Loads Data from a User-fed CSV Path, if CSV passed\n",
    "    def load_data(self):\n",
    "        if self.data_path is None:\n",
    "            print(\"No data path preloaded. Downloading Fear and Greed and BTC data...\")\n",
    "            self.data = fetch_fear_and_greed_btc()\n",
    "        else:\n",
    "            print(\"Data path preloaded. saving csv to dataframe...\")\n",
    "            self.data = pd.read_csv(self.data_path, parse_dates=True, index_col='timestamp') \n",
    "\n",
    "    def preprocess_data(self):\n",
    "        (\n",
    "            self.X_train_scaled,\n",
    "            self.X_test_scaled,\n",
    "            self.y_train_scaled,\n",
    "            self.y_test_scaled,\n",
    "            self.X_test,\n",
    "            self.y_test,\n",
    "            self.dates_train,\n",
    "            self.dates_test,\n",
    "            self.features_test_df\n",
    "        ) = self.preprocessor.preprocess_data(self.data)\n",
    "        print(self.X_train_scaled.shape)\n",
    "    \n",
    "    def reshape_for_lstm(self):\n",
    "        self.X_train_scaled = self.X_train_scaled.reshape((self.X_train_scaled.shape[0], self.X_train_scaled.shape[1], self.X_train_scaled.shape[2]))\n",
    "        self.X_test_scaled = self.X_test_scaled.reshape((self.X_test_scaled.shape[0], self.X_test_scaled.shape[1], self.X_test_scaled.shape[2]))\n",
    "\n",
    "    def build_model_lstm(self):\n",
    "        self.reshape_for_lstm()\n",
    "        timesteps = self.X_train_scaled.shape[1] \n",
    "        features = self.X_train_scaled.shape[2] \n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(timesteps, features)))\n",
    "        model.add(LSTM(517, return_sequences=False))\n",
    "        model.add(Dropout(0.4808067231743268)) # Dropout Regularization\n",
    "        # model.add(LSTM(120, return_sequences=False))\n",
    "        # model.add(Dropout(0.23702192322434543)) # Dropout Regularization\n",
    "        model.add(Dense(36, activation='relu'))\n",
    "        # model.add(BatchNormalization()) # Batch Normalization\n",
    "        # model.add(Dense(10, activation='relu'))\n",
    "        model.add(Dense(1))  # No activation for regression\n",
    "        model.compile(optimizer=Adam(learning_rate=self.learning_rate), loss=self.loss, metrics=self.metrics)\n",
    "        model.summary()\n",
    "        self.model = model\n",
    "        save_and_visualize_model(self.model)\n",
    "\n",
    "    def train_model(self):\n",
    "        self.history = self.model.fit(\n",
    "            self.X_train_scaled, \n",
    "            self.y_train_scaled, \n",
    "            epochs = self.epochs, \n",
    "            batch_size = self.batch_size, \n",
    "            validation_split = self.validation_split, \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    def plot_training_history(self):\n",
    "        plot_loss_training_history(self.history)\n",
    "        plot_mae_training_history(self.history)\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        self.evaluator = ModelEvaluator(self.model, self.X_test, self.y_test, self.X_test_scaled, self.y_test_scaled, self.y_scaler)\n",
    "        self.evaluator.evaluate_model()\n",
    "        # self.evaluator.atr_to_data()\n",
    "    \n",
    "    def predict_model(self):\n",
    "        self.predictions_inversed = self.evaluator.predict_model()\n",
    "        print(self.predictions_inversed.shape)\n",
    "\n",
    "    def save_model(self):  \n",
    "        self.model_path = f'{self.current_timestamp}_LSTM_model_epochs_{self.epochs}.keras'\n",
    "        self.model.save(self.model_path)\n",
    "        print(\"Model saved successfully.\")\n",
    "\n",
    "    def generate_model_signals(self):\n",
    "        self.X_test = generate_signal(self.features_test_df, self.predictions_inversed, self.dates_test)\n",
    "        # print(self.X_test)\n",
    "\n",
    "    def backtest_signals(self):\n",
    "        run_backtest(data=self.X_test, plot=self.plot)\n",
    "\n",
    "    def run_and_train(self):\n",
    "        self.load_data()\n",
    "        self.preprocess_data()\n",
    "        self.build_model_lstm()\n",
    "        self.train_model()\n",
    "        self.plot_training_history()\n",
    "        self.evaluate_model()\n",
    "        self.predict_model()\n",
    "        self.save_model()\n",
    "        self.generate_model_signals()\n",
    "        self.backtest_signals()\n",
    "\n",
    "    def run_with_pretrained(self):\n",
    "        self.load_data()\n",
    "        self.preprocess_data()\n",
    "        self.reshape_for_lstm()\n",
    "        self.evaluate_model()\n",
    "        self.predict_model()\n",
    "        self.generate_model_signals()\n",
    "        self.backtest_signals()\n",
    "\n",
    "\n",
    "model = LSTMModel(#model_path = 'backtests/Low_MAE/backtest_3/2024_07_08_01_38_10_LSTM_model_epochs_145.keras',\n",
    "                  #data_path='fear_greed_btc_combined.csv',\n",
    "                  test_size=0.25, \n",
    "                  learning_rate=0.0005514365217126862, \n",
    "                  epochs=50, \n",
    "                  batch_size=122, \n",
    "                  validation_split=0.25, \n",
    "                  plot=True)\n",
    "\n",
    "model.run_and_train()\n",
    "# model.run_with_pretrained()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the optimizer code. Run this to fine tune your model's hyper parameters and get better predicitve capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from keras.backend import clear_session\n",
    "from keras.layers import Input, LSTM, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "data = fetch_fear_and_greed_btc()\n",
    "X_scaler = RobustScaler()\n",
    "y_scaler = RobustScaler()\n",
    "preprocessor = DataPreprocessor(X_scaler, y_scaler)\n",
    "\n",
    "BATCHSIZE = 64\n",
    "VALIDATION_SPLIT = 0.25\n",
    "CLASSES = 10\n",
    "EPOCHS = 200\n",
    "\n",
    "def objective(trial):\n",
    "    # Clear clutter from previous Keras session graphs.\n",
    "    clear_session()\n",
    "\n",
    "    X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, _, _, _, _, _ = preprocessor.preprocess_data(data)\n",
    "\n",
    "    # Print shapes for debugging\n",
    "    print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
    "    print(f\"X_test_scaled shape: {X_test_scaled.shape}\")\n",
    "\n",
    "    X_train_scaled = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], X_train_scaled.shape[2])) \n",
    "    X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], X_test_scaled.shape[2]))\n",
    "\n",
    "    timesteps = X_train_scaled.shape[1] \n",
    "    features = X_train_scaled.shape[2]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(timesteps, features)))\n",
    "    model.add(LSTM(units=trial.suggest_int('LSTM Neurons_0', 10, 200), return_sequences=True))\n",
    "    model.add(Dropout(trial.suggest_float('Dropout Rate_0', .0001, .50)))\n",
    "    # model.add(LSTM(units=trial.suggest_int('LSTM Neurons_1', 10, 1000), return_sequences=False))\n",
    "    # model.add(Dropout(trial.suggest_float('Dropout Rate_1 ', .0001, .50)))\n",
    "    model.add(Dense(trial.suggest_int('Dense Neurons', 1, 50), activation='relu'))\n",
    "    model.add(Dense(1))  # No activation for regression\n",
    "\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    model.compile(\n",
    "        loss='mean_squared_error',\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        metrics=['mean_absolute_error']\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train_scaled,\n",
    "        y_train_scaled,\n",
    "        # validation_data=(X_test_scaled, y_test_scaled),\n",
    "        shuffle=False,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCHSIZE,\n",
    "        validation_split=VALIDATION_SPLIT,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # Evaluate the model accuracy on the validation set.\n",
    "    score = model.evaluate(X_test_scaled, y_test_scaled, verbose=0)\n",
    "    return score[1]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    \n",
    "    study.optimize(objective, n_trials=10, timeout=100_000)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is another instance of the LSTMModel Class, using example optimization parameters above. You can change this as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "class LSTMModel:\n",
    "    def __init__(self, model_path=None, data_path=None, lags=5, test_size=.25, learning_rate = 0.001, epochs=50, batch_size=32, validation_split=0.2, plot=True):\n",
    "        self.model = None\n",
    "        self.model_path = model_path\n",
    "        self.history = None\n",
    "        self.data_path = data_path\n",
    "        self.lag_features = ['value', 'Close'] # change these if you want to calculate lags on different feature columns\n",
    "        self.target_col = 'Close' # change this if you want to target a different variable than Close\n",
    "        self.X_scaler = RobustScaler()\n",
    "        self.y_scaler = RobustScaler()\n",
    "        self.preprocessor = DataPreprocessor(self.X_scaler, self.y_scaler, self.lag_features, lags, self.target_col, test_size)\n",
    "        self.current_timestamp = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "        self.learning_rate = learning_rate \n",
    "        self.loss = 'mean_squared_error' # change this if you're not going to solve for a regression target\n",
    "        self.metrics = ['mean_absolute_error']  # change this if you're not going to solve for a regression target\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_split = validation_split\n",
    "\n",
    "        self.plot = plot # set plot to false when instantiating if you dont want the backtest graph\n",
    "\n",
    "        if model_path:\n",
    "            self.load_saved_model(model_path)\n",
    "    \n",
    "    def load_saved_model(self, model_path):\n",
    "        self.model = load_model(model_path)\n",
    "        print(f'Model loaded from {model_path}')\n",
    "\n",
    "    # Loads Data from a User-fed CSV Path, if CSV passed\n",
    "    def load_data(self):\n",
    "        if self.data_path is None:\n",
    "            print(\"No data path preloaded. Downloading Fear and Greed and BTC data...\")\n",
    "            self.data = fetch_fear_and_greed_btc()\n",
    "        else:\n",
    "            print(\"Data path preloaded. saving csv to dataframe...\")\n",
    "            self.data = pd.read_csv(self.data_path, parse_dates=True, index_col='timestamp') \n",
    "\n",
    "    def preprocess_data(self):\n",
    "        (\n",
    "            self.X_train_scaled,\n",
    "            self.X_test_scaled,\n",
    "            self.y_train_scaled,\n",
    "            self.y_test_scaled,\n",
    "            self.X_test,\n",
    "            self.y_test,\n",
    "            self.dates_train,\n",
    "            self.dates_test,\n",
    "            self.features_test_df\n",
    "        ) = self.preprocessor.preprocess_data(self.data)\n",
    "        print(self.X_train_scaled.shape)\n",
    "    \n",
    "    def reshape_for_lstm(self):\n",
    "        self.X_train_scaled = self.X_train_scaled.reshape((self.X_train_scaled.shape[0], self.X_train_scaled.shape[1], self.X_train_scaled.shape[2]))\n",
    "        self.X_test_scaled = self.X_test_scaled.reshape((self.X_test_scaled.shape[0], self.X_test_scaled.shape[1], self.X_test_scaled.shape[2]))\n",
    "\n",
    "    def build_model_lstm(self):\n",
    "        self.reshape_for_lstm()\n",
    "        timesteps = self.X_train_scaled.shape[1] \n",
    "        features = self.X_train_scaled.shape[2] \n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(timesteps, features)))\n",
    "        model.add(LSTM(517, return_sequences=False))\n",
    "        model.add(Dropout(0.4808067231743268)) # Dropout Regularization\n",
    "        # model.add(LSTM(120, return_sequences=False))\n",
    "        # model.add(Dropout(0.23702192322434543)) # Dropout Regularization\n",
    "        model.add(Dense(36, activation='relu'))\n",
    "        # model.add(BatchNormalization()) # Batch Normalization\n",
    "        # model.add(Dense(10, activation='relu'))\n",
    "        model.add(Dense(1))  # No activation for regression\n",
    "        model.compile(optimizer=Adam(learning_rate=self.learning_rate), loss=self.loss, metrics=self.metrics)\n",
    "        model.summary()\n",
    "        self.model = model\n",
    "        save_and_visualize_model(self.model)\n",
    "\n",
    "    def train_model(self):\n",
    "        self.history = self.model.fit(\n",
    "            self.X_train_scaled, \n",
    "            self.y_train_scaled, \n",
    "            epochs = self.epochs, \n",
    "            batch_size = self.batch_size, \n",
    "            validation_split = self.validation_split, \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    def plot_training_history(self):\n",
    "        plot_loss_training_history(self.history)\n",
    "        plot_mae_training_history(self.history)\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        self.evaluator = ModelEvaluator(self.model, self.X_test, self.y_test, self.X_test_scaled, self.y_test_scaled, self.y_scaler)\n",
    "        self.evaluator.evaluate_model()\n",
    "        # self.evaluator.atr_to_data()\n",
    "    \n",
    "    def predict_model(self):\n",
    "        self.predictions_inversed = self.evaluator.predict_model()\n",
    "        print(self.predictions_inversed.shape)\n",
    "\n",
    "    def save_model(self):  \n",
    "        self.model_path = f'{self.current_timestamp}_LSTM_model_epochs_{self.epochs}.keras'\n",
    "        self.model.save(self.model_path)\n",
    "        print(\"Model saved successfully.\")\n",
    "\n",
    "    def generate_model_signals(self):\n",
    "        self.X_test = generate_signal(self.features_test_df, self.predictions_inversed, self.dates_test)\n",
    "        # print(self.X_test)\n",
    "\n",
    "    def backtest_signals(self):\n",
    "        run_backtest(data=self.X_test, plot=self.plot)\n",
    "\n",
    "    def run_and_train(self):\n",
    "        self.load_data()\n",
    "        self.preprocess_data()\n",
    "        self.build_model_lstm()\n",
    "        self.train_model()\n",
    "        self.plot_training_history()\n",
    "        self.evaluate_model()\n",
    "        self.predict_model()\n",
    "        self.save_model()\n",
    "        self.generate_model_signals()\n",
    "        self.backtest_signals()\n",
    "\n",
    "    def run_with_pretrained(self):\n",
    "        self.load_data()\n",
    "        self.preprocess_data()\n",
    "        self.reshape_for_lstm()\n",
    "        self.evaluate_model()\n",
    "        self.predict_model()\n",
    "        self.generate_model_signals()\n",
    "        self.backtest_signals()\n",
    "\n",
    "\n",
    "model = LSTMModel(#model_path = 'backtests/Low_MAE/backtest_3/2024_07_08_01_38_10_LSTM_model_epochs_145.keras',\n",
    "                  #data_path='fear_greed_btc_combined.csv',\n",
    "                  test_size=0.25, \n",
    "                  learning_rate=0.0005514365217126862, \n",
    "                  epochs=145, \n",
    "                  batch_size=122, \n",
    "                  validation_split=0.25, \n",
    "                  plot=True)\n",
    "\n",
    "model.run_and_train()\n",
    "# model.run_with_pretrained()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fear_greed_lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
